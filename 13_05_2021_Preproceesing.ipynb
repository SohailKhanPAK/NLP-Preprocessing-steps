{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "13/05/2021 Preproceesing",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN/fx18UKiytl+5sTHF6i6e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SohailKhanPAK/NLP-Preprocessing-steps/blob/main/13_05_2021_Preproceesing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8axqjQkAOCb",
        "outputId": "58c8bad7-020d-45f4-8182-1dd3297d6c3c"
      },
      "source": [
        "pip install autocorrect"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: autocorrect in /usr/local/lib/python3.7/dist-packages (2.5.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sXgDfrFARy8",
        "outputId": "f857456e-7acc-479e-a186-bbe6dacc717f"
      },
      "source": [
        "pip install contractions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.0.48)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.21)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tslc7IZAJda",
        "outputId": "159b8f97-8bab-40f4-8a3b-e32efdfda6cd"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd \n",
        "import nltk \n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import os\n",
        "import re\n",
        "from autocorrect import Speller\n",
        "from nltk import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import contractions\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_upwlerAxio"
      },
      "source": [
        "pathFile1 = '/content/DataSet new - Sohail Version.xlsx'\n",
        "pathFile2 = '/content/df_new new updated dataset.xlsx'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0INqzExSB6w1"
      },
      "source": [
        "\n",
        "\n",
        "> **Preprocessing Steps**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gel0bUovB_V1"
      },
      "source": [
        "\n",
        "\n",
        "> **Defining Functions**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKYDKreDB6Nu"
      },
      "source": [
        "#1) lower_case_convertion\n",
        "\n",
        "def lower_case_convertion(text):\n",
        "\tlower_text = text.lower()\n",
        "\treturn lower_text\n",
        "\n",
        "#2) Removing Numbers\n",
        "\n",
        "def remove_numbers(text):\n",
        "\tnumber_pattern = r'\\d+'\n",
        "\twithout_number = re.sub(pattern=number_pattern,\n",
        " repl=\" \", string=text)\n",
        "\treturn without_number\n",
        "\n",
        "\n",
        "#3) Removing Punctuations\n",
        "from string import punctuation\n",
        "\n",
        "def remove_punctuation(text):\n",
        "\treturn text.translate(str.maketrans('', '', punctuation))\n",
        "  \n",
        "#4) Spelling Correction\n",
        "\n",
        "def spell_autocorrect(text):\n",
        "\tcorrect_spell_words = []\n",
        "\tspell_corrector = Speller(lang='en')\n",
        "\tfor word in word_tokenize(text):\n",
        "\t\tcorrect_word = spell_corrector(word)\n",
        "\t\tcorrect_spell_words.append(correct_word)\n",
        "\tcorrect_spelling = ' '.join(correct_spell_words)\n",
        "\treturn correct_spelling\n",
        "\n",
        "\n",
        "#5)  Contractions\n",
        "def contractions_function(data):\n",
        "  expanded_words = []  \n",
        "  tweets_contraction = []\n",
        "  for string in data:\n",
        "    expanded_words = []\n",
        "    for word in string.split():\n",
        "      expanded_words.append(contractions.fix(word))\n",
        "      expanded_text = ' '.join(expanded_words)\n",
        "    tweets_contraction.append(expanded_text)\n",
        "  return tweets_contraction\n",
        "\n",
        "def remove_emojis(text):\n",
        "\temoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"  # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "\n",
        "\twithout_emoji = emoji_pattern.sub(r'',text)\n",
        "\treturn without_emoji\n",
        "\n",
        "def listToString(s):   \n",
        "    str1 = \" \" \n",
        "    return (str1.join(s)) \n",
        "  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ih-yM1wNUjMz",
        "outputId": "f0118082-e780-4af1-b8ff-8ea50d70ac2e"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "def lemmatization(text):\n",
        "  from nltk.stem import WordNetLemmatizer\n",
        "  from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\t# word tokenization\n",
        "  tokens = word_tokenize(text)\n",
        "  for index in range(len(tokens)):\n",
        "    # lemma word\n",
        "    lemma_word = lemma.lemmatize(tokens[index])\n",
        "    tokens[index] = lemma_word\n",
        "  return ' '.join(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29QU6F89AVQ7"
      },
      "source": [
        "def Thesis_2_Preprocessing(path):\n",
        "\n",
        "  #loading file\n",
        "  df=pd.read_excel(f'{path}')\n",
        "\n",
        "  #initilize X (Tweets)\n",
        "  df_tweets = df['Tweets']\n",
        "\n",
        "  #                             **** Applying Preprocessing Steps ****\n",
        "  \n",
        "  #1) Convert All text to lowerCase\n",
        "  converting_lower_case =     [lower_case_convertion(x.replace('   ','  ').replace('  ',' ').strip()) for x in df_tweets ]\n",
        "  print(\"1) Lower Case Conversion step is Completed\")\n",
        "\n",
        "  #2) Remove Numbers\n",
        "  removing_numbers =          [remove_numbers(x)      for x in converting_lower_case]\n",
        "  print(\"2) Removing Numbers step is Completed\")\n",
        "\n",
        "  #3) Removing Punctuations\n",
        "  removing_punctuations=      [remove_punctuation(x)  for x in removing_numbers ]\n",
        "  print(\"3) Removing Punctuations step is Completed\")\n",
        "\n",
        "  #4) removing emojis\n",
        "  removing_emojis =           [remove_emojis(x)       for x in removing_punctuations]\n",
        "  print(\"4) Removing Emojis step is Completed\")\n",
        "\n",
        "\n",
        "  #5) contractions\n",
        "  data_with_contractions =    contractions_function(removing_emojis)\n",
        "  print(\"5) Contraction step is Completed\")\n",
        "\n",
        "  #6) Spelling Correction\n",
        "  spellingCorrection =        [spell_autocorrect(x)    for x in data_with_contractions ]\n",
        "  print(\"6) Spelling Correction step is Completed\")\n",
        "  \n",
        "  #6 word_lemitizer\n",
        "  Lemitized_data =[ lemmatization(x) for x in spellingCorrection]\n",
        "  print(\"6) Lemmatizer step is Completed\")\n",
        " \n",
        "  return Lemitized_data\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnNrz444FRYq",
        "outputId": "c31a0037-d9de-4fda-fa85-20ec2e582cd4"
      },
      "source": [
        "data=Thesis_2_Preprocessing(pathFile1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1) Lower Case Conversion step is Completed\n",
            "2) Removing Numbers step is Completed\n",
            "3) Removing Punctuations step is Completed\n",
            "4) Removing Emojis step is Completed\n",
            "5) Contraction step is Completed\n",
            "6) Spelling Correction step is Completed\n",
            "6) Lemmatizer step is Completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FErxjzGladZK",
        "outputId": "447be8f3-8de9-487f-fd4a-9277eb6b0a00"
      },
      "source": [
        "data2=Thesis_2_Preprocessing(pathFile2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1) Lower Case Conversion step is Completed\n",
            "2) Removing Numbers step is Completed\n",
            "3) Removing Punctuations step is Completed\n",
            "4) Removing Emojis step is Completed\n",
            "5) Contraction step is Completed\n",
            "6) Spelling Correction step is Completed\n",
            "6) Lemmatizer step is Completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtxRY78Wab3K"
      },
      "source": [
        "preprocessed_tweets_new = data\n",
        "preprocessed_tweets_old = data2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEhhOcaAr2TK"
      },
      "source": [
        "df_new= pd.read_excel(pathFile1)\n",
        "df_old = pd.read_excel(pathFile2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9dATI2xrno7",
        "outputId": "3a788466-4360-4b31-9874-b8eb523c430f"
      },
      "source": [
        "!pip install xlsxwriter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xlsxwriter in /usr/local/lib/python3.7/dist-packages (1.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TS20svztwVcn"
      },
      "source": [
        "xdvimport pandas as pd\n",
        "preprocessed_tweets_new = pd.DataFrame( data={\"Tweets Category\":list(df_new['Tweet Category']),\"ProductName\":list(df_new['ProductName']),\"PreProceesed tweets\":preprocessed_tweets_new ,\"F/NF\":list(df_new['F/NF']),\"Feature Name\":list(df_new['Feature Name'])} )\n",
        "\n",
        "# Use pandas to write the  file in excel format\n",
        "\n",
        "preprocessed_tweets_new.to_excel(\"new dataset preprocessed.xlsx\",sheet_name='preprocessed dataset')  \n",
        "\n",
        "\n",
        "writer = pd.ExcelWriter(\"new dataset preprocessed.xlsx\", engine='xlsxwriter')\n",
        "preprocessed_tweets_new.to_excel(writer,sheet_name = 'preprocessed dataset', index=False)\n",
        "writer.save() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3Gz6bcVRAuD"
      },
      "source": [
        "**Wrtting text to excel format**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBVJM2Rurm4j"
      },
      "source": [
        "import pandas as pd\n",
        "preprocessed_tweets_old = pd.DataFrame( data={\"Tweets Category\":list(df_old['Tweet Category']),\"Product Name\":list(df_old['Product Name']),\"PreProceesed tweets\":preprocessed_tweets_old ,\"F/NF\":list(df_old['F/NF']),\"Feature Name\":list(df_old['Feature Name'])} )\n",
        "\n",
        "# Use pandas to write the  file in excel format\n",
        "\n",
        "preprocessed_tweets_old.to_excel(\"old dataset preprocessed.xlsx\",sheet_name='preprocessed dataset')  \n",
        "\n",
        "\n",
        "writer = pd.ExcelWriter(\"old dataset preprocessed.xlsx\", engine='xlsxwriter')\n",
        "preprocessed_tweets_old.to_excel(writer,sheet_name = 'preprocessed dataset', index=False)\n",
        "writer.save() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdwkEMfSjB3K",
        "outputId": "bbd09e94-4565-475e-896c-1911c0802f18"
      },
      "source": [
        "data2=Thesis_2_Preprocessing(pathFile2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1) Lower Case Conversion step is Completed\n",
            "2) Removing Numbers step is Completed\n",
            "3) Removing Punctuations step is Completed\n",
            "4) Removing Emojis step is Completed\n",
            "5) Contraction step is Completed\n",
            "6) Spelling Correction step is Completed\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}